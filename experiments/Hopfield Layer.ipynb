{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "E5dbakk-Yxau"
   },
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.0-dev20200804\n"
     ]
    }
   ],
   "source": [
    "# !pip install --upgrade -q git+https://github.com/shuiruge/neural-ode.git@master\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from node.core import get_dynamical_node_function\n",
    "from node.solvers.runge_kutta import RKF56Solver\n",
    "from node.solvers.dynamical_runge_kutta import DynamicalRKF56Solver\n",
    "from node.hopfield import StopCondition\n",
    "\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "J0QNHlhGAPX2"
   },
   "outputs": [],
   "source": [
    "IMAGE_SIZE = (32, 32)\n",
    "# IMAGE_SIZE = (8, 8)  # XXX: test!\n",
    "BINARIZE = True\n",
    "# BINARIZE = False\n",
    "\n",
    "\n",
    "def pooling(x, size):\n",
    "    # x shape: [None, width, height]\n",
    "    x = tf.expand_dims(x, axis=-1)\n",
    "    x = tf.image.resize(x, size)\n",
    "    return x  # shape: [None, size[0], size[1], 1]\n",
    "\n",
    "\n",
    "def process_data(X, y, image_size, binarize):\n",
    "    X = pooling(X, image_size)\n",
    "    X = X / 255.\n",
    "    if binarize:\n",
    "        X = tf.where(X < 0.5, -1., 1.)\n",
    "    else:\n",
    "        X = X * 2 - 1\n",
    "    X = tf.reshape(X, [-1, image_size[0] * image_size[1]])\n",
    "    y = tf.one_hot(y, 10)\n",
    "    return tf.cast(X, tf.float32), tf.cast(y, tf.float32)\n",
    "\n",
    "\n",
    "def evaluate(model, X, y):\n",
    "    yhat = model.predict(X)\n",
    "    acc = np.mean(np.argmax(y, axis=-1) == np.argmax(yhat, axis=-1))\n",
    "    return acc\n",
    "\n",
    "\n",
    "(x_train, y_train), _ = tf.keras.datasets.mnist.load_data()\n",
    "x_train, y_train = process_data(x_train, y_train, IMAGE_SIZE, BINARIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sign(x):\n",
    "    return tf.where(x > 0, 1., -1.)\n",
    "\n",
    "\n",
    "class NonidentityRecon(tf.keras.layers.Layer):\n",
    "    \"\"\"Base class of re-constructor which is further constrainted\n",
    "    to avoid learning to be an identity map.\"\"\"\n",
    "\n",
    "\n",
    "class DenseRecon(NonidentityRecon):\n",
    "    \"\"\"Fully connected non-identity re-constructor.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    activation : callable\n",
    "    binarize: callable, optional\n",
    "        Binarization method for non-training process. If `None`, then no\n",
    "        binarization.\n",
    "    use_bias : bool, optional\n",
    "        For simplicity, bias is not employed by default.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 activation,\n",
    "                 binarize=None,\n",
    "                 use_bias=False,\n",
    "                 **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.activation = activation\n",
    "        self.binarize = binarize\n",
    "        self.use_bias = use_bias\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        depth = input_shape[-1]\n",
    "        self._dense = tf.keras.layers.Dense(\n",
    "            units=depth,\n",
    "            activation=self.activation,\n",
    "            use_bias=self.use_bias,\n",
    "            kernel_constraint=symmetrize_and_mask_diagonal,\n",
    "        )\n",
    "        super().build(input_shape)\n",
    "\n",
    "    def call(self, x, training=None):\n",
    "        y = self._dense(x)\n",
    "        if training:\n",
    "            return y\n",
    "        if self.binarize is not None:\n",
    "            y = self.binarize(y)\n",
    "        return y\n",
    "\n",
    "\n",
    "def symmetrize_and_mask_diagonal(kernel):\n",
    "    \"\"\"Symmetric kernel with vanishing diagonal.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    kernel : tensor\n",
    "        Shape (N, N) for a positive integer N.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tensor\n",
    "        The shape and dtype as the input.\n",
    "    \"\"\"\n",
    "    w = (kernel + tf.transpose(kernel)) / 2\n",
    "    w = tf.linalg.set_diag(w, tf.zeros(kernel.shape[0:-1]))\n",
    "    return w\n",
    "\n",
    "\n",
    "class Conv2dRecon(NonidentityRecon):\n",
    "    \"\"\"Cellular automata based non-identity re-constructor.\n",
    "\n",
    "    References\n",
    "    ----------\n",
    "    1. Cellular automata as convolutional neural networks (arXiv: 1809.02942).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    filters : int\n",
    "    kernel_size : int\n",
    "    activation : callable\n",
    "    binarize: callable, optional\n",
    "        Binarization method for non-training process. If `None`, then no\n",
    "        binarization.\n",
    "    flatten : bool, optional\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 filters,\n",
    "                 kernel_size,\n",
    "                 activation,\n",
    "                 binarize=None,\n",
    "                 flatten=False,\n",
    "                 **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.filters = int(filters)\n",
    "        self.kernel_size = int(kernel_size)\n",
    "        self.activation = activation\n",
    "        self.binarize = binarize\n",
    "        self.flatten = flatten\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        recon_layers = [\n",
    "            tf.keras.layers.Conv2D(\n",
    "                filters=self.filters,\n",
    "                kernel_size=self.kernel_size,\n",
    "                activation='relu',\n",
    "                padding='same',\n",
    "                kernel_constraint=mask_center,\n",
    "            ),\n",
    "            tf.keras.layers.Conv2D(1, 1, activation=self.activation),\n",
    "        ]\n",
    "        if self.flatten:\n",
    "            depth = input_shape[-1]\n",
    "            two_dim_shape = [int(np.sqrt(depth))] * 2 + [1]\n",
    "            recon_layers.insert(0, tf.keras.layers.Reshape(two_dim_shape))\n",
    "            recon_layers.append(tf.keras.layers.Reshape([depth]))\n",
    "        self._recon = tf.keras.Sequential(recon_layers)\n",
    "        self._recon.build(input_shape)\n",
    "\n",
    "        super().build(input_shape)\n",
    "\n",
    "    def call(self, x, training=None):\n",
    "        y = self._recon(x)\n",
    "        if training:\n",
    "            return y\n",
    "        if self.binarize is not None:\n",
    "            y = self.binarize(y)\n",
    "        return y\n",
    "\n",
    "\n",
    "def _get_center_mask(dim: int) -> np.array:\n",
    "    assert dim % 2 == 1\n",
    "    center = int(dim / 2) + 1\n",
    "    mask = np.ones([dim, dim, 1, 1])\n",
    "    mask[center, center, 0, 0] = 0\n",
    "    return mask\n",
    "\n",
    "\n",
    "def mask_center(kernel):\n",
    "    # kernel shape: [dim, dim, n_channels, n_filters]\n",
    "    dim, *_ = kernel.get_shape().as_list()\n",
    "    mask = tf.constant(_get_center_mask(dim), dtype=kernel.dtype)\n",
    "    return kernel * mask\n",
    "\n",
    "\n",
    "class ContinuousTimeHopfieldLayer(tf.keras.layers.Layer):\n",
    "\n",
    "    def __init__(self,\n",
    "                 non_identity_recon: NonidentityRecon,\n",
    "                 tau=1,\n",
    "                 static_solver=RKF56Solver(\n",
    "                     dt=1e-1, tol=1e-3, min_dt=1e-2),\n",
    "                 dynamical_solver=DynamicalRKF56Solver(\n",
    "                     dt=1e-1, tol=1e-3, min_dt=1e-2),\n",
    "                 max_time=1e+3,\n",
    "                 relax_tol=1e-3,\n",
    "                 reg_factor=1e-0,\n",
    "                 **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.non_identity_recon = non_identity_recon\n",
    "        self.tau = float(tau)\n",
    "        self.static_solver = static_solver\n",
    "        self.dynamical_solver = dynamical_solver\n",
    "        self.max_time = float(max_time)\n",
    "        self.relax_tol = float(relax_tol)\n",
    "        self.reg_factor = float(reg_factor)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        f = self.non_identity_recon\n",
    "\n",
    "        def dynamics(t, x):\n",
    "            return (-x + f(x)) / self.tau\n",
    "\n",
    "        stop_condition = StopCondition(dynamics, self.max_time, self.relax_tol)\n",
    "        node_fn = get_dynamical_node_function(\n",
    "          self.dynamical_solver, self.static_solver, dynamics, stop_condition)\n",
    "\n",
    "        self._stop_condition = stop_condition\n",
    "        self._node_fn = node_fn\n",
    "\n",
    "        super().build(input_shape)\n",
    "\n",
    "    def call(self, x, training=None):\n",
    "        if training:\n",
    "            r = self.non_identity_recon(x)\n",
    "            loss = tf.reduce_mean(tf.abs(x - r))\n",
    "            self.add_loss(self.reg_factor * loss)\n",
    "            return r\n",
    "        else:\n",
    "            t0 = tf.constant(0.)\n",
    "            return self._node_fn(t0, x)\n",
    "\n",
    "\n",
    "class DiscreteTimeHopfieldLayer(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    References\n",
    "    ----------\n",
    "    1. Information Theory, Inference, and Learning Algorithm (D. Mackay),\n",
    "       chapter 42.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    non_identity_recon : NonidentityRecon\n",
    "    max_steps : int\n",
    "    async_ratio : float, optional\n",
    "        Percentage of \"bits\" to be randomly masked in each updation.\n",
    "    relax_tol : float, optional\n",
    "    reg_factor : float, optional\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    final_step : int32 scalar\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 non_identity_recon: NonidentityRecon,\n",
    "                 max_steps: int,\n",
    "                 async_ratio=0.,\n",
    "                 relax_tol=1e-3,\n",
    "                 reg_factor=1e-0,\n",
    "                 **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.non_identity_recon = non_identity_recon\n",
    "        self.max_steps = max_steps\n",
    "        self.async_ratio = float(async_ratio)\n",
    "        self.relax_tol = float(relax_tol)\n",
    "        self.reg_factor = float(reg_factor)\n",
    "\n",
    "        self.final_step = tf.Variable(0, trainable=False)\n",
    "\n",
    "    @tf.function\n",
    "    def _update(self, x):\n",
    "        y = self.non_identity_recon(x, training=False)\n",
    "        if self.async_ratio > 0:\n",
    "            # mask has no batch dim\n",
    "            mask = tf.where(\n",
    "                tf.random.uniform(y.shape[1:]) < self.async_ratio,\n",
    "                0., 1.)\n",
    "            y *= mask[tf.newaxis, ...]\n",
    "        return y\n",
    "\n",
    "    def call(self, x, training=None):\n",
    "        if training:\n",
    "            r = self.non_identity_recon(x, training=True)\n",
    "            loss = tf.reduce_mean(tf.abs(x - r))\n",
    "            self.add_loss(self.reg_factor * loss)\n",
    "            return r\n",
    "\n",
    "        else:\n",
    "            for step in tf.range(self.max_steps):\n",
    "                next_x = self._update(x)\n",
    "                if diff(next_x, x) < self.relax_tol:\n",
    "                    break\n",
    "                x = next_x\n",
    "            self.final_step.assign(step + 1)\n",
    "            return x\n",
    "\n",
    "\n",
    "def diff(x, y):\n",
    "    return tf.reduce_max(tf.abs(x - y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(model_type):\n",
    "    if model_type == 'continuous_dense':\n",
    "        model = tf.keras.Sequential([\n",
    "            CTHLayer(DenseRecon(activation=tf.tanh),\n",
    "                     reg_factor=1),\n",
    "        ])\n",
    "    elif model_type == 'continuous_cnn':\n",
    "        model = tf.keras.Sequential([\n",
    "            CTHLayer(Conv2dRecon(filters=16, kernel_size=5, activation=tf.tanh,\n",
    "                                 flatten=True),\n",
    "                     reg_factor=1),\n",
    "        ])\n",
    "    elif model_type == 'discrete_dense':\n",
    "        model = tf.keras.Sequential([\n",
    "            DTHLayer(DenseRecon(activation=tf.tanh, binarize=sign),\n",
    "                     max_steps=100,\n",
    "                     reg_factor=1),\n",
    "        ])\n",
    "    elif model_type == 'discrete_cnn':\n",
    "        model = tf.keras.Sequential([\n",
    "            DTHLayer(Conv2dRecon(filters=16, kernel_size=5,\n",
    "                                 activation=tf.tanh, binarize=sign,\n",
    "                                 flatten=True),\n",
    "                     max_steps=100,\n",
    "                     reg_factor=1),\n",
    "        ])\n",
    "    else:\n",
    "        raise ValueError()\n",
    "    model.compile(optimizer='adam')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "colab_type": "code",
    "id": "kbqJL6p9GjLe",
    "outputId": "e92d7d0f-3a91-42fe-c48a-9b051c9b0ea8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7813/7813 [==============================] - 39s 5ms/step - loss: 0.0376\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f9fe45ab990>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = create_model('discrete_dense')\n",
    "X = x_train[:100].numpy()\n",
    "ds0 = tf.data.Dataset.from_tensor_slices(X)\n",
    "ds = ds0.shuffle(10000).repeat(10000).batch(128)\n",
    "model.fit(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dth_layer (DTHLayer)         (None, 1024)              1048577   \n",
      "=================================================================\n",
      "Total params: 1,048,577\n",
      "Trainable params: 1,048,576\n",
      "Non-trainable params: 1\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relax steps: 6\n",
      "2.0 => 0.0\n"
     ]
    }
   ],
   "source": [
    "# noised_X = X + np.random.normal(size=X.shape) * 0.1\n",
    "noised_X = np.where(np.random.random(size=X.shape) < 0.3, -X, X)\n",
    "recon_X = model.predict(noised_X)\n",
    "\n",
    "try:\n",
    "    print('Relax time:', model.layers[-1]._stop_condition.relax_time.numpy())\n",
    "except Exception:\n",
    "    pass\n",
    "try:\n",
    "    print('Relax steps:', model.layers[-1].final_step.numpy())\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "orig_err = noised_X - X\n",
    "err = recon_X - X\n",
    "print(f'{np.quantile(np.abs(orig_err), 0.99)} => '\n",
    "      f'{np.quantile(np.abs(err), 0.99)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions and Discussions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resource Occupations\n",
    "\n",
    "#### Time\n",
    "\n",
    "1. Dense version is much faster than CNN version.\n",
    "\n",
    "#### Space\n",
    "\n",
    "1. CNN version needs only ~ 10^2 parameters. Recall that dense version needs 10^7 parameters.\n",
    "\n",
    "1. To reduce the number of variables in the dense version, use [prunning](https://stackoverflow.com/a/56451791/1218716) after training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### De-noising\n",
    "\n",
    "1. However, CNN version is not robust to bit-flipping. Dense version is still very robust to it. Bit-flipping fails for CNN version hints that the information is not sparsely (distributedly) stored. Thus it cannot re-construct the original bit only from the information stored in its local neighbors. (Notice that bit-flipping creates non-smooth, thus always great, differences.) To see this, run the re-constructor on the bit-flipping noised inputs to see the 0.99-quantile of the re-construction error, comparing for both dense and CNN versions.\n",
    "\n",
    "1. Dense version gains 99% re-construction even for 40% bit-flipping."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binarization\n",
    "\n",
    "1. Binarization is also essential to CNN version. Non-binarized inputs won't de-noise. The essense of binarization maybe traced to the simplicity it leads to. Indeed, the final loss without binarization will be greater (0.03X -> 0.04X).\n",
    "\n",
    "1. Change X in {-1, 1} to {0, 1} causes error in de-noising. Don't know why."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discrete Time\n",
    "\n",
    "1. Discrete time version is much much faster in predicting. Without lossing the attributes the continuous version has\n",
    "\n",
    "1. Async update decreases the performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discrete State\n",
    "\n",
    "1. Discrete time when using discrete time improves performance significantly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "1. Cellular automata as convolutional neural networks (arXiv: 1809.02942)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "EHL.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
