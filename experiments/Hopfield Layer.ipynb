{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "E5dbakk-Yxau"
   },
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.0-dev20200901\n"
     ]
    }
   ],
   "source": [
    "# !pip install --upgrade -q git+https://github.com/shuiruge/neural-ode.git@master\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from node.core import get_dynamical_node_function\n",
    "from node.solvers.runge_kutta import RKF56Solver\n",
    "from node.solvers.dynamical_runge_kutta import DynamicalRKF56Solver\n",
    "from node.hopfield import StopCondition\n",
    "\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "J0QNHlhGAPX2"
   },
   "outputs": [],
   "source": [
    "IMAGE_SIZE = (32, 32)\n",
    "# IMAGE_SIZE = (8, 8)  # XXX: test!\n",
    "BINARIZE = True\n",
    "# BINARIZE = False\n",
    "\n",
    "\n",
    "def pooling(x, size):\n",
    "    # x shape: [None, width, height]\n",
    "    x = tf.expand_dims(x, axis=-1)\n",
    "    x = tf.image.resize(x, size)\n",
    "    return x  # shape: [None, size[0], size[1], 1]\n",
    "\n",
    "\n",
    "def process_data(X, y, image_size, binarize):\n",
    "    X = pooling(X, image_size)\n",
    "    X = X / 255.\n",
    "    if binarize:\n",
    "        X = tf.where(X < 0.5, -1., 1.)\n",
    "    else:\n",
    "        X = X * 2 - 1\n",
    "    X = tf.reshape(X, [-1, image_size[0] * image_size[1]])\n",
    "    y = tf.one_hot(y, 10)\n",
    "    return tf.cast(X, tf.float32), tf.cast(y, tf.float32)\n",
    "\n",
    "\n",
    "def evaluate(model, X, y):\n",
    "    yhat = model.predict(X)\n",
    "    acc = np.mean(np.argmax(y, axis=-1) == np.argmax(yhat, axis=-1))\n",
    "    return acc\n",
    "\n",
    "\n",
    "(x_train, y_train), _ = tf.keras.datasets.mnist.load_data()\n",
    "x_train, y_train = process_data(x_train, y_train, IMAGE_SIZE, BINARIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sign(x):\n",
    "    return tf.where(x > 0, 1., -1.)\n",
    "\n",
    "\n",
    "class NonidentityRecon(tf.keras.layers.Layer):\n",
    "    \"\"\"Base class of re-constructor which is further constrainted\n",
    "    to avoid learning to be an identity map.\"\"\"\n",
    "\n",
    "\n",
    "class DenseRecon(NonidentityRecon):\n",
    "    \"\"\"Fully connected non-identity re-constructor.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    activation : callable\n",
    "    binarize: callable, optional\n",
    "        Binarization method for non-training process. If `None`, then no\n",
    "        binarization.\n",
    "    use_bias : bool, optional\n",
    "        For simplicity, bias is not employed by default.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 activation,\n",
    "                 binarize=None,\n",
    "                 use_bias=False,\n",
    "                 **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.activation = activation\n",
    "        self.binarize = binarize\n",
    "        self.use_bias = use_bias\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        depth = input_shape[-1]\n",
    "        self._recon = tf.keras.layers.Dense(\n",
    "            units=depth,\n",
    "            activation=self.activation,\n",
    "            use_bias=self.use_bias,\n",
    "            kernel_constraint=symmetrize_and_mask_diagonal,\n",
    "        )\n",
    "        super().build(input_shape)\n",
    "\n",
    "    def call(self, x, training=None):\n",
    "        y = self._recon(x)\n",
    "        if training:\n",
    "            return y\n",
    "        if self.binarize is not None:\n",
    "            y = self.binarize(y)\n",
    "        return y\n",
    "\n",
    "\n",
    "def symmetrize_and_mask_diagonal(kernel):\n",
    "    \"\"\"Symmetric kernel with vanishing diagonal.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    kernel : tensor\n",
    "        Shape (N, N) for a positive integer N.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tensor\n",
    "        The shape and dtype as the input.\n",
    "    \"\"\"\n",
    "    w = (kernel + tf.transpose(kernel)) / 2\n",
    "    w = tf.linalg.set_diag(w, tf.zeros(kernel.shape[0:-1]))\n",
    "    return w\n",
    "\n",
    "\n",
    "class ModernDenseRecon(NonidentityRecon):\n",
    "    \"\"\"\n",
    "    References\n",
    "    ----------\n",
    "    1. https://ml-jku.github.io/hopfield-layers/\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    activation : callable\n",
    "    binarize: callable, optional\n",
    "        Binarization method for non-training process. If `None`, then no\n",
    "        binarization.\n",
    "    n : int, optional\n",
    "        Number of patterns to learn. If `None`, then `memory` shall be\n",
    "        provided.\n",
    "    memory : array-like, optional\n",
    "        Shape `[n, depth]` where `n` is the same as the argument `n`,\n",
    "        and `depth` is the dimension of inputs. If `None`, then `n`\n",
    "        shall be provided.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 activation,\n",
    "                 binarize=None,\n",
    "                 n=None,\n",
    "                 memory=None,\n",
    "                 **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        \n",
    "        if n is None and memory is None:\n",
    "            raise ValueError('Either `n` or `memory` has to be provided.')\n",
    "\n",
    "        self.activation = activation\n",
    "        self.binarize = binarize\n",
    "        self.n = n\n",
    "        self.memory = memory\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        depth = input_shape[-1]\n",
    "        if self.memory is None:\n",
    "            n = self.n\n",
    "            initializer = 'glorot_uniform'\n",
    "        else:\n",
    "            n = self.memory.shape[0]\n",
    "\n",
    "            def initializer(_, dtype=None):\n",
    "                return tf.constant(self.memory, dtype=dtype)\n",
    "\n",
    "        self.kernel = self.add_weight(name='kernel',\n",
    "                                      shape=[n, depth],\n",
    "                                      initializer=initializer,\n",
    "                                      trainable=True)\n",
    "        super().build(input_shape)\n",
    "        \n",
    "    def _recon(self, x):\n",
    "        f = self.activation\n",
    "        W = self.kernel\n",
    "        return f(x @ tf.transpose(W)) @ W\n",
    "\n",
    "    def call(self, x, training=None):\n",
    "        y = self._recon(x)\n",
    "        if training:\n",
    "            return y\n",
    "        if self.binarize is not None:\n",
    "            y = self.binarize(y)\n",
    "        return y\n",
    "\n",
    "\n",
    "class Conv2dRecon(NonidentityRecon):\n",
    "    \"\"\"Cellular automata based non-identity re-constructor.\n",
    "\n",
    "    References\n",
    "    ----------\n",
    "    1. Cellular automata as convolutional neural networks (arXiv: 1809.02942).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    filters : int\n",
    "    kernel_size : int\n",
    "    activation : callable\n",
    "    binarize: callable, optional\n",
    "        Binarization method for non-training process. If `None`, then no\n",
    "        binarization.\n",
    "    flatten : bool, optional\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 filters,\n",
    "                 kernel_size,\n",
    "                 activation,\n",
    "                 binarize=None,\n",
    "                 flatten=False,\n",
    "                 **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.filters = int(filters)\n",
    "        self.kernel_size = int(kernel_size)\n",
    "        self.activation = activation\n",
    "        self.binarize = binarize\n",
    "        self.flatten = flatten\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        recon_layers = [\n",
    "            tf.keras.layers.Conv2D(\n",
    "                filters=self.filters,\n",
    "                kernel_size=self.kernel_size,\n",
    "                activation='relu',\n",
    "                padding='same',\n",
    "                kernel_constraint=mask_center,\n",
    "            ),\n",
    "            tf.keras.layers.Conv2D(1, 1, activation=self.activation),\n",
    "        ]\n",
    "        if self.flatten:\n",
    "            depth = input_shape[-1]\n",
    "            two_dim_shape = [int(np.sqrt(depth))] * 2 + [1]\n",
    "            recon_layers.insert(0, tf.keras.layers.Reshape(two_dim_shape))\n",
    "            recon_layers.append(tf.keras.layers.Reshape([depth]))\n",
    "        self._recon = tf.keras.Sequential(recon_layers)\n",
    "        self._recon.build(input_shape)\n",
    "\n",
    "        super().build(input_shape)\n",
    "\n",
    "    def call(self, x, training=None):\n",
    "        y = self._recon(x)\n",
    "        if training:\n",
    "            return y\n",
    "        if self.binarize is not None:\n",
    "            y = self.binarize(y)\n",
    "        return y\n",
    "\n",
    "\n",
    "def _get_center_mask(dim: int) -> np.array:\n",
    "    assert dim % 2 == 1\n",
    "    center = int(dim / 2) + 1\n",
    "    mask = np.ones([dim, dim, 1, 1])\n",
    "    mask[center, center, 0, 0] = 0\n",
    "    return mask\n",
    "\n",
    "\n",
    "def mask_center(kernel):\n",
    "    # kernel shape: [dim, dim, n_channels, n_filters]\n",
    "    dim, *_ = kernel.get_shape().as_list()\n",
    "    mask = tf.constant(_get_center_mask(dim), dtype=kernel.dtype)\n",
    "    return kernel * mask\n",
    "\n",
    "\n",
    "class ContinuousTimeHopfieldLayer(tf.keras.layers.Layer):\n",
    "\n",
    "    def __init__(self,\n",
    "                 non_identity_recon: NonidentityRecon,\n",
    "                 tau=1,\n",
    "                 static_solver=RKF56Solver(\n",
    "                     dt=1e-1, tol=1e-3, min_dt=1e-2),\n",
    "                 dynamical_solver=DynamicalRKF56Solver(\n",
    "                     dt=1e-1, tol=1e-3, min_dt=1e-2),\n",
    "                 max_time=1e+3,\n",
    "                 relax_tol=1e-3,\n",
    "                 reg_factor=1e-0,\n",
    "                 **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.non_identity_recon = non_identity_recon\n",
    "        self.tau = float(tau)\n",
    "        self.static_solver = static_solver\n",
    "        self.dynamical_solver = dynamical_solver\n",
    "        self.max_time = float(max_time)\n",
    "        self.relax_tol = float(relax_tol)\n",
    "        self.reg_factor = float(reg_factor)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        f = self.non_identity_recon\n",
    "\n",
    "        def dynamics(t, x):\n",
    "            return (-x + f(x)) / self.tau\n",
    "\n",
    "        stop_condition = StopCondition(dynamics, self.max_time, self.relax_tol)\n",
    "        node_fn = get_dynamical_node_function(\n",
    "          self.dynamical_solver, self.static_solver, dynamics, stop_condition)\n",
    "\n",
    "        self._stop_condition = stop_condition\n",
    "        self._node_fn = node_fn\n",
    "\n",
    "        super().build(input_shape)\n",
    "\n",
    "    def _learn(self, x):\n",
    "        r = self.non_identity_recon(x)\n",
    "        loss = tf.reduce_mean(tf.abs(x - r))\n",
    "        return r, loss\n",
    "\n",
    "    def _update(self, x):\n",
    "        t0 = tf.constant(0.)\n",
    "        return self._node_fn(t0, x)\n",
    "\n",
    "    def call(self, x, training=None):\n",
    "        if training:\n",
    "            y, loss = self._learn(x)\n",
    "            self.add_loss(self.reg_factor * loss)\n",
    "        else:\n",
    "            y = self._update(x)\n",
    "        return y\n",
    "\n",
    "\n",
    "class DiscreteTimeHopfieldLayer(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    References\n",
    "    ----------\n",
    "    1. Information Theory, Inference, and Learning Algorithm (D. Mackay),\n",
    "       chapter 42.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    non_identity_recon : NonidentityRecon\n",
    "    max_steps : int\n",
    "    async_ratio : float, optional\n",
    "        Percentage of \"bits\" to be randomly masked in each updation.\n",
    "    relax_tol : float, optional\n",
    "    reg_factor : float, optional\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    final_step : int32 scalar\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 non_identity_recon: NonidentityRecon,\n",
    "                 max_steps: int,\n",
    "                 async_ratio=0.,\n",
    "                 relax_tol=1e-3,\n",
    "                 reg_factor=1e-0,\n",
    "                 **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.non_identity_recon = non_identity_recon\n",
    "        self.max_steps = max_steps\n",
    "        self.async_ratio = float(async_ratio)\n",
    "        self.relax_tol = float(relax_tol)\n",
    "        self.reg_factor = float(reg_factor)\n",
    "\n",
    "        self.final_step = tf.Variable(0, trainable=False)\n",
    "\n",
    "    def _learn(self, x):\n",
    "        r = self.non_identity_recon(x, training=True)\n",
    "        loss = tf.reduce_mean(tf.abs(x - r))\n",
    "        return r, loss\n",
    "\n",
    "    def _update(self, x):\n",
    "        for step in tf.range(self.max_steps):\n",
    "            next_x = self._update_step(x)\n",
    "            if diff(next_x, x) < self.relax_tol:\n",
    "                break\n",
    "            x = next_x\n",
    "        self.final_step.assign(step)\n",
    "        return x\n",
    "\n",
    "    def _update_step(self, x):\n",
    "        y = self.non_identity_recon(x, training=False)\n",
    "        if self.async_ratio > 0:\n",
    "            # mask has no batch dim\n",
    "            mask = tf.where(\n",
    "                tf.random.uniform(y.shape[1:]) < self.async_ratio,\n",
    "                0., 1.)\n",
    "            y *= mask[tf.newaxis, ...]\n",
    "        return y\n",
    "\n",
    "    def call(self, x, training=None):\n",
    "        if training:\n",
    "            y, loss = self._learn(x)\n",
    "            self.add_loss(self.reg_factor * loss)\n",
    "        else:\n",
    "            y = self._update(x)\n",
    "        return y\n",
    "\n",
    "\n",
    "def diff(x, y):\n",
    "    return tf.reduce_max(tf.abs(x - y))\n",
    "\n",
    "\n",
    "def tempered(T, fn):\n",
    "    \"\"\"Converts f(x) to f(x/T).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    T : float\n",
    "        The temperature.\n",
    "    fn : callable\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    callable\n",
    "    \"\"\"\n",
    "    T = float(T) \n",
    "    return lambda x: fn(x / T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(model_type: str):\n",
    "    if model_type == 'continuous_dense':\n",
    "        model = tf.keras.Sequential([\n",
    "            ContinuousTimeHopfieldLayer(\n",
    "                DenseRecon(activation=tf.tanh),\n",
    "                reg_factor=1),\n",
    "        ])\n",
    "    elif model_type == 'continuous_cnn':\n",
    "        model = tf.keras.Sequential([\n",
    "            ContinuousTimeHopfieldLayer(\n",
    "                Conv2dRecon(\n",
    "                    filters=16,\n",
    "                    kernel_size=5,\n",
    "                    activation=tf.tanh,\n",
    "                    flatten=True,\n",
    "                ),\n",
    "                reg_factor=1),\n",
    "        ])\n",
    "    elif model_type == 'discrete_dense':\n",
    "        model = tf.keras.Sequential([\n",
    "            DiscreteTimeHopfieldLayer(\n",
    "                DenseRecon(\n",
    "                    activation=tf.tanh,\n",
    "                    binarize=sign,\n",
    "                ),\n",
    "                max_steps=100,\n",
    "                reg_factor=1),\n",
    "        ])\n",
    "    elif model_type == 'discrete_cnn':\n",
    "        model = tf.keras.Sequential([\n",
    "            DiscreteTimeHopfieldLayer(\n",
    "                Conv2dRecon(\n",
    "                    filters=128,\n",
    "                    kernel_size=3,\n",
    "                    activation=tf.tanh,\n",
    "                    binarize=sign,\n",
    "                    flatten=True,\n",
    "                ),\n",
    "                max_steps=100,\n",
    "                reg_factor=1),\n",
    "        ])\n",
    "    elif model_type == 'discrete_modern_dense':\n",
    "        model = tf.keras.Sequential([\n",
    "            DiscreteTimeHopfieldLayer(\n",
    "                ModernDenseRecon(\n",
    "                    memory=x_train[:100],\n",
    "                    #n=100,\n",
    "                    activation=tempered(0.1, tf.nn.softmax),\n",
    "                    binarize=sign,\n",
    "                ),\n",
    "                max_steps=100,\n",
    "                reg_factor=1),\n",
    "        ])\n",
    "    elif model_type == 'continuous_modern_dense':\n",
    "        model = tf.keras.Sequential([\n",
    "            ContinuousTimeHopfieldLayer(\n",
    "                ModernDenseRecon(\n",
    "                    memory=x_train[:100],\n",
    "                    #n=100,\n",
    "                    activation=tempered(0.1, tf.nn.softmax),\n",
    "                    binarize=sign,\n",
    "                ),\n",
    "                reg_factor=1),\n",
    "        ])\n",
    "    else:\n",
    "        raise ValueError(f'Unknown model type: \"{model_type}\".')\n",
    "    model.compile(optimizer='adam')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "colab_type": "code",
    "id": "kbqJL6p9GjLe",
    "outputId": "e92d7d0f-3a91-42fe-c48a-9b051c9b0ea8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7813/7813 [==============================] - 44s 6ms/step - loss: 0.0000e+00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f84a5758bd0>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = create_model('discrete_modern_dense')\n",
    "X = x_train[:100].numpy()\n",
    "ds0 = tf.data.Dataset.from_tensor_slices(X)\n",
    "ds = ds0.shuffle(10000).repeat(10000).batch(128)\n",
    "model.fit(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "discrete_time_hopfield_layer (None, 1024)              102401    \n",
      "=================================================================\n",
      "Total params: 102,401\n",
      "Trainable params: 102,400\n",
      "Non-trainable params: 1\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relax steps: 1\n",
      "2.0 => 0.0\n"
     ]
    }
   ],
   "source": [
    "# noised_X = X + np.random.normal(size=X.shape) * 0.2\n",
    "noised_X = np.where(np.random.random(size=X.shape) < 0.2, -X, X)\n",
    "recon_X = model.predict(noised_X)\n",
    "\n",
    "try:\n",
    "    print('Relax time:', model.layers[-1]._stop_condition.relax_time.numpy())\n",
    "except Exception:\n",
    "    pass\n",
    "try:\n",
    "    print('Relax steps:', model.layers[-1].final_step.numpy())\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "orig_err = noised_X - X\n",
    "err = recon_X - X\n",
    "print(f'{np.quantile(np.abs(orig_err), 0.995)} => '\n",
    "      f'{np.quantile(np.abs(err), 0.995)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions and Discussions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resource Occupations\n",
    "\n",
    "#### Time\n",
    "\n",
    "1. Dense version is much faster than CNN version.\n",
    "\n",
    "#### Space\n",
    "\n",
    "1. CNN version needs only ~ 10^2 parameters. Recall that dense version needs 10^7 parameters.\n",
    "\n",
    "1. To reduce the number of variables in the dense version, use [prunning](https://stackoverflow.com/a/56451791/1218716) after training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### De-noising\n",
    "\n",
    "1. However, CNN version is not robust to bit-flipping. Dense version is still very robust to it. Bit-flipping fails for CNN version hints that the information is not sparsely (distributedly) stored. Thus it cannot re-construct the original bit only from the information stored in its local neighbors. (Notice that bit-flipping creates non-smooth, thus always great, differences.) To see this, run the re-constructor on the bit-flipping noised inputs to see the 0.99-quantile of the re-construction error, comparing for both dense and CNN versions. Increasing filters will not change the failure.\n",
    "\n",
    "1. Dense version gains 99% re-construction even for 40% bit-flipping."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binarization\n",
    "\n",
    "1. Binarization is also essential to CNN version. Non-binarized inputs won't de-noise. The essense of binarization maybe traced to the simplicity it leads to. Indeed, the final loss without binarization will be greater (0.03X -> 0.04X).\n",
    "\n",
    "1. Change X in {-1, 1} to {0, 1} causes error in de-noising. Don't know why."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discrete Time\n",
    "\n",
    "1. Discrete time version is much much faster in predicting. Without lossing the attributes the continuous version has\n",
    "\n",
    "1. Async update decreases the performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discrete State\n",
    "\n",
    "1. Discrete time when using discrete time improves performance significantly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relation between Continuous and Discrete Time\n",
    "\n",
    "The continous time version, i.e. ODE version, and the discrete time version, i.e. iterative equation version, are related, since both of them can be regarded as root-finding. That is, find the root $x_{\\star}$ s.t. $x - f(x) = 0$. This root is the fixed point, or relaxition phase. The ODE version uses the gradient descent method, and the iterative equation version uses definition. Both of them will find one of the many roots, which is ensured by the Lyapunov functions of them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modern Version\n",
    "\n",
    "1. If the kernel is initialized by giving memory, the learning is redundant, and the performance is, as expected, splendid.\n",
    "\n",
    "1. However, if the kernel is initialized by Glorot initializer as usual, the learning is hard, and the performance is terrible.\n",
    "\n",
    "1. If the memory is a subset of the training data, then performance is terrible again.\n",
    "\n",
    "1. If the weight is pruned, then the memory is destroyed too. However, this will not happen to the \"traditional\" version."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "1. Cellular automata as convolutional neural networks (arXiv: 1809.02942).\n",
    "\n",
    "1. Blog: [Hopfield Networks is All You Need](https://ml-jku.github.io/hopfield-layers/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "EHL.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
