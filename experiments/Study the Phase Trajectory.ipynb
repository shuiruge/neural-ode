{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 2.44640827\n",
      "10 1.79321373\n",
      "20 1.5027082\n",
      "30 1.51360345\n",
      "40 1.46247411\n",
      "50 1.24658418\n",
      "60 1.00590777\n",
      "70 0.648667455\n",
      "80 0.665576041\n",
      "90 0.944899738\n",
      "100 0.636870742\n",
      "110 0.717926\n",
      "120 0.528918326\n",
      "130 0.545190871\n",
      "140 0.484120131\n",
      "150 0.51841867\n",
      "160 0.436444908\n",
      "170 0.257186174\n",
      "180 0.501385272\n",
      "190 0.473104894\n",
      "200 0.386331707\n",
      "210 0.439368129\n",
      "220 0.415830791\n",
      "230 0.371607542\n",
      "240 0.452593982\n",
      "250 0.478032589\n",
      "260 0.494715452\n",
      "270 0.461615682\n",
      "280 0.355713934\n",
      "290 0.401416779\n",
      "300 0.3571136\n",
      "310 0.460269451\n",
      "320 0.431369036\n",
      "330 0.563382506\n",
      "340 0.270792365\n",
      "350 0.540035188\n",
      "360 0.465415478\n",
      "370 0.370710969\n",
      "380 0.337132603\n",
      "390 0.364546686\n",
      "400 0.519720078\n",
      "410 0.226486772\n",
      "420 0.277364701\n",
      "430 0.29386735\n",
      "440 0.31721893\n",
      "450 0.417874634\n",
      "460 0.130583793\n",
      "470 0.475861669\n",
      "480 0.283348173\n",
      "490 0.394779623\n",
      "500 0.310987741\n",
      "510 0.399646819\n",
      "520 0.299859494\n",
      "530 0.485628366\n",
      "540 0.341901809\n",
      "550 0.242458194\n",
      "560 0.594061136\n",
      "570 0.594908237\n",
      "580 0.503781199\n",
      "590 0.264123291\n",
      "600 0.346892059\n",
      "610 0.251698017\n",
      "620 0.284000784\n",
      "630 0.428739101\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from node.base import get_node_function\n",
    "from node.fix_grid import RKSolver\n",
    "\n",
    "\n",
    "# for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def dsigmoid(x):\n",
    "    return tf.nn.sigmoid(x) * (1 - tf.nn.sigmoid(x))\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def inv_sigmoid(x):\n",
    "    return tf.math.log(x + 1e-8) - tf.math.log(1 - x + 1e-8)\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def softmax(x, axis):\n",
    "    return tf.nn.log_softmax(x, axis)\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def softmin(x, axis):\n",
    "    return -tf.nn.log_softmax(-x, axis)\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def softrescale(x, axis):\n",
    "    max = softmax(x, axis)\n",
    "    min = softmin(x, axis)\n",
    "    return (x - min) / (max - min)\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def rescale(x, axis):\n",
    "    max = tf.reduce_max(x, axis, keepdims=True)\n",
    "    min = tf.reduce_min(x, axis, keepdims=True)\n",
    "    return (x - min) / (max - min)\n",
    "\n",
    "\n",
    "input_dim = 28 * 28\n",
    "network = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(32, activation='relu'),\n",
    "    # tanh output for bounding the scale of phase vector field\n",
    "    tf.keras.layers.Dense(input_dim, activation='tanh'),\n",
    "])\n",
    "network.build([None, input_dim])\n",
    "\n",
    "\n",
    "class MyLayer(tf.keras.layers.Layer):\n",
    "\n",
    "    def __init__(self, network, dt, num_grids, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.network = network\n",
    "        self.dt = dt\n",
    "        self.num_grids = num_grids\n",
    "\n",
    "        t0 = tf.constant(0.)\n",
    "        self.tN = t0 + num_grids * dt\n",
    "\n",
    "#         def pvf(t, x):\n",
    "#             r\"\"\"\n",
    "#             $x^{\\prime} = \\sigma\\left(\\sigma^{-1}(x) + \\Delta t f(t, x; \\theta) \\right)$,\n",
    "#             element-wisely.\n",
    "#             \"\"\"\n",
    "#             return dsigmoid(inv_sigmoid(x)) * self.network(x)\n",
    "\n",
    "        def pvf(t, x):\n",
    "            with tf.GradientTape() as g:\n",
    "                g.watch(x)\n",
    "                f = self.network(x)\n",
    "                r = rescale(x, axis=-1)\n",
    "            return g.gradient(r, [x], [f])[0]\n",
    "\n",
    "        self._pvf = pvf\n",
    "        self._node_fn = get_node_function(RKSolver(self.dt), 0., pvf)\n",
    "\n",
    "    def call(self, x):\n",
    "        y = self._node_fn(self.tN, x)\n",
    "        return y\n",
    "\n",
    "\n",
    "def process(X, y):\n",
    "    X = X / 255.\n",
    "    X = tf.reshape(X, [-1, 28 * 28])\n",
    "    y = tf.one_hot(y, 10)\n",
    "    return tf.cast(X, tf.float32), tf.cast(y, tf.float32)\n",
    "\n",
    "\n",
    "mnist = tf.keras.datasets.mnist\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train, y_train = process(x_train, y_train)\n",
    "x_test, y_test = process(x_test, y_test)\n",
    "\n",
    "my_layer = MyLayer(network, dt=1e-2, num_grids=100)\n",
    "output_layer = tf.keras.layers.Dense(\n",
    "    10, activation='softmax',\n",
    "    kernel_regularizer=tf.keras.regularizers.l2(1.))\n",
    "model = tf.keras.Sequential([my_layer, output_layer])\n",
    "model.build([None, 28 * 28])\n",
    "\n",
    "optimizer = tf.compat.v1.train.AdamOptimizer()\n",
    "loss_fn = tf.losses.CategoricalCrossentropy()\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def train_one_step(x, y):\n",
    "    with tf.GradientTape() as tape:\n",
    "        outputs = model(x)\n",
    "        loss = loss_fn(y, outputs)\n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "    return loss\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def train(dataset):\n",
    "    step, loss, reg = 0, float('inf'), float('inf')\n",
    "    for x, y in dataset:\n",
    "        loss = train_one_step(x, y)\n",
    "        if step % 10 == 0:\n",
    "            tf.print(step, loss)\n",
    "        step += 1\n",
    "    return loss\n",
    "\n",
    "\n",
    "def clip(min, max, x):\n",
    "    min = np.ones_like(x) * min\n",
    "    max = np.ones_like(x) * max\n",
    "    x = np.where(x < min, min, x)\n",
    "    x = np.where(x > max, max, x)\n",
    "    return x\n",
    "\n",
    "\n",
    "num_epochs = 3\n",
    "batch_size = 128\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "dataset = dataset.repeat(num_epochs).batch(batch_size)\n",
    "\n",
    "train(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from node.utils import tracer\n",
    "\n",
    "def flip(array, ratio):\n",
    "    is_flipped = np.random.random(size=array.shape) < ratio\n",
    "    return np.where(is_flipped, 1 - array, array)\n",
    "\n",
    "t0 = 0.\n",
    "t1 = 1.\n",
    "dt = 1e-2\n",
    "traj_size = int((t1 - t0) / dt) + 1\n",
    "\n",
    "n_data = 20\n",
    "flip_ratio = 0.1\n",
    "\n",
    "involved_labels = {1, 3, 5}\n",
    "\n",
    "i, data, labels = 0, [], []\n",
    "for x, y in zip(x_train, y_train):\n",
    "    y = np.argmax(y)\n",
    "    if y not in involved_labels:\n",
    "        continue\n",
    "    data.append(x)\n",
    "    labels.append(y)\n",
    "    data.append(flip(x, flip_ratio))\n",
    "    labels.append(y)\n",
    "    i += 1\n",
    "    if i == n_data:\n",
    "        break\n",
    "data = np.array(data)\n",
    "labels = np.array(labels)\n",
    "\n",
    "trace = tracer(RKSolver(1e-2), my_layer._pvf)\n",
    "trajectories = trace(t0, t1, dt, data)\n",
    "trajectories = tf.transpose(trajectories, [1, 0, 2])\n",
    "trajectories = trajectories.numpy()\n",
    "\n",
    "\n",
    "def get_trajectory(x):\n",
    "    \"\"\"Input shape `[28 * 28]`, output shape `[frames, 28, 28]`.\"\"\"\n",
    "    trajectory = trace(t0, t1, dt, [x]).numpy()[:,0,:]\n",
    "    trajectory = np.reshape(trajectory, [28, 28])\n",
    "    return trajectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = output_layer(trajectories[:,-1,:]).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 10\n",
    "print(preds[i])\n",
    "labels[i], np.argmax(preds[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import animation\n",
    "\n",
    "\n",
    "def rescale(array):\n",
    "    shape = array.shape\n",
    "    y = np.reshape(array, [-1])\n",
    "    y = (y - np.min(y)) / (np.max(y) - np.min(y))\n",
    "    return np.reshape(y, shape)\n",
    "\n",
    "\n",
    "def visualize_trajectory(trajectory):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        trajectory: np.array\n",
    "            Shape `[frames, x_pixal, y_pixal]`.\n",
    "    \n",
    "    Returns: animation.FuncAnimation\n",
    "    \"\"\"\n",
    "    fig = plt.figure()\n",
    "    ax = plt.axes()\n",
    "    img = ax.imshow(trajectory[0], cmap='gray')\n",
    "\n",
    "    def init():\n",
    "        img.set_data([[]])\n",
    "        return img,\n",
    "\n",
    "    def animate(i):\n",
    "        y = rescale(trajectory[i])\n",
    "        img.set_data(y)\n",
    "        return img,\n",
    "\n",
    "    anim = animation.FuncAnimation(\n",
    "        fig, animate, init_func=init, frames=traj_size, blit=True)\n",
    "    return anim\n",
    "\n",
    "\n",
    "for i, trajectory in enumerate(trajectories):\n",
    "    label = labels[i]\n",
    "    anim = visualize_trajectory(trajectory.reshape([-1, 28, 28]))\n",
    "    anim.save(f'../dat/trajectory/anim_i{i}_l{label}.mp4')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "### Approach 1\n",
    "\n",
    "~~1. Animation plotting shows that attractors exist for all the displayed instances.~~\n",
    "\n",
    "~~1. By tracing the flipping ratio, we find that, while setting $\\tilde{L} = 3$ in the training process, the flip ratio decreases from $\\sim 0.1$ to $\\sim 0.001$ only after $\\tilde{L} > 30$ approximately for all trials. That is, the static phase vector field is trained without reaching the attractors. And when reaching the attractors, instances in the same class have little difference (but not vanishing), instances from different classes become evidently more distinct.~~\n",
    "\n",
    "~~1. The attractors for the same class, even though close to each other, are far from single. It seems to confirm the conclusion in the study of Hebbian learning that high-dimensional dynamic systems have extremely many attractors.~~\n",
    "\n",
    "1. After re-scaling by `lambda x: (x - min(x)) / (max(x) - min(x))`, there does exists attractors having the properties described above. So, it seems that the phase point flying straightly towords some direction specific for different classes. Or say, \"attracted to the direction\".\n",
    "\n",
    "1. It seems that we encountered the chaos along the phase trajectory.\n",
    "\n",
    "### Approach 2\n",
    "\n",
    "$x^{\\prime} = \\sigma\\left(\\sigma^{-1}(x) + \\Delta t f(t, x; \\theta) \\right)$\n",
    "\n",
    "1. Attractors are reached at $L \\sim 300$.\n",
    "1. There are quite a lot of attractors.\n",
    "1. Indeed, in this case, the $x^{\\alpha} = 0, 1$ for $\\forall \\alpha$ are attractors.\n",
    "1. However, this approach introduces an artificial area of fixed points (i.e. $x^{\\alpha} = 0, 1$ for $\\forall \\alpha$), which is not what we hope for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References:\n",
    "\n",
    "1. [Chaos appears in RGE (as a high dimensional non-linear ODE)](https://physics.stackexchange.com/a/55057) (the [paper](https://arxiv.org/abs/hep-th/0304178) related)."
   ]
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
