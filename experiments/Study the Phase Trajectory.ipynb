{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from node.base import get_node_function\n",
    "from node.fix_grid import RKSolver\n",
    "\n",
    "\n",
    "# for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.DEBUG)\n",
    "\n",
    "\n",
    "def log_calling(fn):\n",
    "    \n",
    "    def test_fn(*args, **kwargs):\n",
    "        logger.debug(f'calling {fn.__name__}')\n",
    "        return fn(*args, **kwargs)\n",
    "    \n",
    "    return test_fn\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def dsigmoid(x):\n",
    "    return tf.nn.sigmoid(x) * (1 - tf.nn.sigmoid(x))\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def inv_sigmoid(x):\n",
    "    return tf.math.log(x + 1e-8) - tf.math.log(1 - x + 1e-8)\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def softmax(x, axis):\n",
    "    return tf.nn.log_softmax(x, axis)\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def softmin(x, axis):\n",
    "    return -tf.nn.log_softmax(-x, axis)\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def softrescale(x, axis):\n",
    "    max = softmax(x, axis)\n",
    "    min = softmin(x, axis)\n",
    "    return (x - min) / (max - min)\n",
    "\n",
    "\n",
    "@log_calling\n",
    "@tf.function\n",
    "def rescale(x, axis):\n",
    "    max = tf.reduce_max(x, axis, keepdims=True)\n",
    "    min = tf.reduce_min(x, axis, keepdims=True)\n",
    "    return (x - min) / (max - min)\n",
    "\n",
    "\n",
    "@log_calling\n",
    "@tf.function\n",
    "def get_accuracy(y_true, y_pred):\n",
    "    output_dtype = y_pred.dtype\n",
    "    y_true = tf.argmax(y_true, axis=-1)\n",
    "    y_pred = tf.argmax(y_pred, axis=-1)\n",
    "    return tf.reduce_mean(tf.cast(tf.equal(y_true, y_pred), output_dtype))\n",
    "\n",
    "\n",
    "class GlorotUniform(tf.keras.initializers.GlorotUniform):\n",
    "    \n",
    "    def __init__(self, scale=None, seed=None):\n",
    "        super().__init__(seed)\n",
    "        self.scale = 1 if scale is None else scale\n",
    "    \n",
    "    def __call__(self, *args, **kwargs):\n",
    "        return self.scale * super().__call__(*args, **kwargs)\n",
    "\n",
    "\n",
    "class MyLayer(tf.keras.layers.Layer):\n",
    "\n",
    "    def __init__(self, network, dt, num_grids, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.network = network\n",
    "        self.dt = dt\n",
    "        self.num_grids = num_grids\n",
    "\n",
    "        t0 = tf.constant(0.)\n",
    "        self.tN = t0 + num_grids * dt\n",
    "\n",
    "#         def pvf(t, x):\n",
    "#             r\"\"\"\n",
    "#             $x^{\\prime} = \\sigma\\left(\\sigma^{-1}(x) + \\Delta t f(t, x; \\theta) \\right)$,\n",
    "#             element-wisely.\n",
    "#             \"\"\"\n",
    "#             return dsigmoid(inv_sigmoid(x)) * self.network(x)\n",
    "\n",
    "        @log_calling\n",
    "        @tf.function\n",
    "        def pvf(t, x):\n",
    "            with tf.GradientTape() as g:\n",
    "                g.watch(x)\n",
    "                f = self.network(x)\n",
    "                r = rescale(x, axis=-1)\n",
    "            return g.gradient(r, x, f)\n",
    "\n",
    "        self._pvf = pvf\n",
    "        self._node_fn = get_node_function(RKSolver(self.dt), 0., pvf)\n",
    "\n",
    "    def call(self, x):\n",
    "        y = self._node_fn(self.tN, x)\n",
    "        return y\n",
    "\n",
    "\n",
    "@log_calling\n",
    "@tf.function\n",
    "def process(X, y=None):\n",
    "    X = tf.cast(X, tf.float32)\n",
    "    X = X / 255.\n",
    "    X = tf.reshape(X, [28 * 28])\n",
    "    if y is None:\n",
    "        return X\n",
    "    else:\n",
    "        y = tf.one_hot(y, 10)\n",
    "        y = tf.cast(y, tf.float32)\n",
    "        return X, y\n",
    "\n",
    "\n",
    "def get_train_fn(model, optimizer,\n",
    "                 num_epochs=5,\n",
    "                 batch_size=128,\n",
    "                 skip_step=50):\n",
    "    loss_fn = tf.losses.CategoricalCrossentropy()\n",
    "\n",
    "    @log_calling\n",
    "    @tf.function\n",
    "    def train_one_step(X, y):\n",
    "        with tf.GradientTape() as tape:\n",
    "            outputs = model(X)\n",
    "            loss = loss_fn(y, outputs)\n",
    "        grads = tape.gradient(loss, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "        accuracy = get_accuracy(y, outputs)\n",
    "        return loss, accuracy\n",
    "\n",
    "    @tf.function\n",
    "    def train(X_train, y_train, X_test, y_test):\n",
    "        num_steps_per_epoch = int(len(X_train) / batch_size)\n",
    "\n",
    "        train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
    "        train_dataset = (train_dataset.map(process)\n",
    "                          .shuffle(10000)\n",
    "                          .repeat(num_epochs)\n",
    "                          .batch(batch_size))\n",
    "        test_dataset = tf.data.Dataset.from_tensor_slices((X_test, y_test))\n",
    "        test_dataset = test_dataset.map(process).batch(batch_size)\n",
    "\n",
    "        @tf.function\n",
    "        def evaluate():\n",
    "            step = 0\n",
    "            total_accuracy = 0.\n",
    "            for X, y in test_dataset:\n",
    "                outputs = model(X)\n",
    "                total_accuracy += get_accuracy(y, outputs)\n",
    "                step += 1\n",
    "            return total_accuracy / tf.cast(step, total_accuracy.dtype)\n",
    "\n",
    "        step = 0\n",
    "        loss = float('inf')\n",
    "        reg = float('inf')\n",
    "        accuracy = 0.\n",
    "        for X, y in train_dataset:\n",
    "            loss, accuracy = train_one_step(X, y)\n",
    "\n",
    "            if step % skip_step == 0:\n",
    "                tf.print(step, loss, accuracy)\n",
    "\n",
    "            if step % num_steps_per_epoch == 0:\n",
    "                tf.print('testing')\n",
    "                test_accuracy = evaluate()\n",
    "                tf.print('test accuracy:', test_accuracy)\n",
    "\n",
    "            step += 1\n",
    "        return loss, accuracy\n",
    "\n",
    "    return train\n",
    "\n",
    "\n",
    "def clip(min, max, x):\n",
    "    min = np.ones_like(x) * min\n",
    "    max = np.ones_like(x) * max\n",
    "    x = np.where(x < min, min, x)\n",
    "    x = np.where(x > max, max, x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 28 * 28\n",
    "network = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(32, activation='relu',\n",
    "                          kernel_initializer=GlorotUniform(0.1)),\n",
    "    # tanh output for bounding the scale of phase vector field\n",
    "    tf.keras.layers.Dense(input_dim, activation='tanh',\n",
    "                          kernel_initializer=GlorotUniform(0.1)),\n",
    "])\n",
    "network.build([None, input_dim])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_datasets(raw_X, raw_y):\n",
    "    involved_labels = {1, 3, 5}\n",
    "\n",
    "    X, y = [], []\n",
    "    for xi, yi in zip(raw_X, raw_y):\n",
    "        if yi in involved_labels:\n",
    "            X.append(xi)\n",
    "            y.append(yi)\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "mnist = tf.keras.datasets.mnist\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "X_train, y_train = get_datasets(x_train, y_train)\n",
    "X_test, y_test = get_datasets(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing\n",
      "0 2.50347924 0\n",
      "test accuracy: 0.0139973955\n",
      "50 1.67159247 0.640625\n",
      "100 1.37097931 0.6796875\n",
      "testing\n",
      "test accuracy: 0.672106\n",
      "150 1.34138274 0.6171875\n",
      "200 1.32991087 0.703125\n",
      "250 1.20285666 0.6875\n",
      "testing\n",
      "test accuracy: 0.701038837\n",
      "300 1.10391653 0.6171875\n",
      "350 1.13243628 0.6953125\n",
      "400 1.05768108 0.671875\n",
      "testing\n",
      "test accuracy: 0.699085712\n",
      "450 1.01933336 0.7421875\n",
      "500 1.04378784 0.734375\n",
      "550 1.05227804 0.6875\n",
      "testing\n",
      "test accuracy: 0.715239286\n",
      "600 0.934368432 0.6953125\n",
      "650 1.0137589 0.7734375\n",
      "700 0.930982113 0.6796875\n",
      "testing\n",
      "test accuracy: 0.702991962\n",
      "750 0.94667387 0.7421875\n",
      "800 0.88119173 0.6640625\n",
      "850 0.860076308 0.6953125\n",
      "testing\n",
      "test accuracy: 0.743153572\n",
      "900 0.893186808 0.703125\n",
      "950 0.808332741 0.734375\n",
      "testing\n",
      "test accuracy: 0.749541461\n",
      "1000 0.862797 0.75\n",
      "1050 0.804313421 0.7265625\n",
      "1100 0.684438 0.9375\n",
      "testing\n",
      "test accuracy: 0.95792383\n",
      "1150 0.475499034 0.953125\n",
      "1200 0.455723584 0.9453125\n",
      "1250 0.451508105 0.953125\n",
      "testing\n",
      "test accuracy: 0.964108706\n",
      "1300 0.427073389 0.984375\n",
      "1350 0.359173298 0.984375\n",
      "1400 0.311591566 0.9765625\n",
      "testing\n",
      "test accuracy: 0.973793805\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: id=2643, shape=(), dtype=float32, numpy=0.42813936>,\n",
       " <tf.Tensor: id=2644, shape=(), dtype=float32, numpy=0.9285714>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_layer = MyLayer(network, dt=1e-1, num_grids=3)\n",
    "output_layer = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(4, activation='relu'),\n",
    "    tf.keras.layers.Dense(10, activation='softmax'),\n",
    "])\n",
    "model = tf.keras.Sequential([my_layer, output_layer])\n",
    "model.build([None, 28 * 28])\n",
    "\n",
    "optimizer = tf.compat.v1.train.AdamOptimizer()\n",
    "\n",
    "train = get_train_fn(model, optimizer, num_epochs=10)\n",
    "train(X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num grids: 1, test accuracy: 0.983171284198761\n",
      "num grids: 3, test accuracy: 0.9938095808029175\n",
      "num grids: 5, test accuracy: 0.9837673902511597\n",
      "num grids: 7, test accuracy: 0.94373619556427\n",
      "num grids: 10, test accuracy: 0.8189998865127563\n",
      "num grids: 15, test accuracy: 0.7044318914413452\n",
      "num grids: 20, test accuracy: 0.6838201880455017\n",
      "num grids: 50, test accuracy: 0.6801632046699524\n",
      "num grids: 100, test accuracy: 0.4737710654735565\n",
      "num grids: 200, test accuracy: 0.6788334250450134\n",
      "num grids: 500, test accuracy: 0.31889674067497253\n",
      "num grids: 1000, test accuracy: 0.35859546065330505\n"
     ]
    }
   ],
   "source": [
    "for num_grids in (1, 3, 5, 7, 10, 15, 20, 50, 100, 200, 500, 1000):\n",
    "    my_layer_2 = MyLayer(network, dt=1e-1, num_grids=num_grids)\n",
    "    model_2 = tf.keras.Sequential([my_layer_2, output_layer])\n",
    "    model_2.build([None, 28 * 28])\n",
    "\n",
    "    @tf.function\n",
    "    def evaluate_2(X, y):\n",
    "        batch_size = 64\n",
    "        num_batches = int(len(X) / batch_size)\n",
    "        dataset = tf.data.Dataset.from_tensor_slices((X, y))\n",
    "        dataset = dataset.map(process).batch(batch_size)\n",
    "\n",
    "        total_accuracy = 0.\n",
    "        for xi, yi in dataset:\n",
    "            outputs = model_2(xi)\n",
    "            total_accuracy += get_accuracy(yi, outputs)\n",
    "        return total_accuracy / tf.cast(num_batches, total_accuracy.dtype)\n",
    "\n",
    "    print(f'num grids: {num_grids}, test accuracy: {evaluate_2(X_test, y_test).numpy()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from node.utils import tracer\n",
    "\n",
    "def flip(array, ratio):\n",
    "    is_flipped = np.random.random(size=array.shape) < ratio\n",
    "    return np.where(is_flipped, 1 - array, array)\n",
    "\n",
    "t0 = 0.\n",
    "t1 = 1.\n",
    "dt = 1e-2\n",
    "traj_size = int((t1 - t0) / dt) + 1\n",
    "\n",
    "n_data = 20\n",
    "flip_ratio = 0.1\n",
    "\n",
    "trace = tracer(RKSolver(1e-2), my_layer._pvf)\n",
    "trajectories = trace(t0, t1, dt, data)\n",
    "trajectories = tf.transpose(trajectories, [1, 0, 2])\n",
    "trajectories = trajectories.numpy()\n",
    "\n",
    "\n",
    "def get_trajectory(x):\n",
    "    \"\"\"Input shape `[28 * 28]`, output shape `[frames, 28, 28]`.\"\"\"\n",
    "    trajectory = trace(t0, t1, dt, [x]).numpy()[:,0,:]\n",
    "    trajectory = np.reshape(trajectory, [28, 28])\n",
    "    return trajectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 10\n",
    "print(preds[i])\n",
    "labels[i], np.argmax(preds[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import animation\n",
    "\n",
    "\n",
    "def rescale(array):\n",
    "    shape = array.shape\n",
    "    y = np.reshape(array, [-1])\n",
    "    y = (y - np.min(y)) / (np.max(y) - np.min(y))\n",
    "    return np.reshape(y, shape)\n",
    "\n",
    "\n",
    "def visualize_trajectory(trajectory):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        trajectory: np.array\n",
    "            Shape `[frames, x_pixal, y_pixal]`.\n",
    "    \n",
    "    Returns: animation.FuncAnimation\n",
    "    \"\"\"\n",
    "    fig = plt.figure()\n",
    "    ax = plt.axes()\n",
    "    img = ax.imshow(trajectory[0], cmap='gray')\n",
    "\n",
    "    def init():\n",
    "        img.set_data([[]])\n",
    "        return img,\n",
    "\n",
    "    def animate(i):\n",
    "        y = rescale(trajectory[i])\n",
    "        img.set_data(y)\n",
    "        return img,\n",
    "\n",
    "    anim = animation.FuncAnimation(\n",
    "        fig, animate, init_func=init, frames=traj_size, blit=True)\n",
    "    return anim\n",
    "\n",
    "\n",
    "for i, trajectory in enumerate(trajectories):\n",
    "    label = labels[i]\n",
    "    anim = visualize_trajectory(trajectory.reshape([-1, 28, 28]))\n",
    "    anim.save(f'../dat/trajectory/anim_i{i}_l{label}.mp4')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "### Approach 1\n",
    "\n",
    "~~1. Animation plotting shows that attractors exist for all the displayed instances.~~\n",
    "\n",
    "~~1. By tracing the flipping ratio, we find that, while setting $\\tilde{L} = 3$ in the training process, the flip ratio decreases from $\\sim 0.1$ to $\\sim 0.001$ only after $\\tilde{L} > 30$ approximately for all trials. That is, the static phase vector field is trained without reaching the attractors. And when reaching the attractors, instances in the same class have little difference (but not vanishing), instances from different classes become evidently more distinct.~~\n",
    "\n",
    "~~1. The attractors for the same class, even though close to each other, are far from single. It seems to confirm the conclusion in the study of Hebbian learning that high-dimensional dynamic systems have extremely many attractors.~~\n",
    "\n",
    "1. After re-scaling by `lambda x: (x - min(x)) / (max(x) - min(x))`, there does exists attractors having the properties described above. So, it seems that the phase point flying straightly towords some direction specific for different classes. Or say, \"attracted to the direction\".\n",
    "\n",
    "1. It seems that we encountered the chaos along the phase trajectory.\n",
    "\n",
    "### Approach 2\n",
    "\n",
    "$x^{\\prime} = \\sigma\\left(\\sigma^{-1}(x) + \\Delta t f(t, x; \\theta) \\right)$\n",
    "\n",
    "1. Attractors are reached at $L \\sim 300$.\n",
    "1. There are quite a lot of attractors.\n",
    "1. Indeed, in this case, the $x^{\\alpha} = 0, 1$ for $\\forall \\alpha$ are attractors.\n",
    "1. However, this approach introduces an artificial area of fixed points (i.e. $x^{\\alpha} = 0, 1$ for $\\forall \\alpha$), which is not what we hope for.\n",
    "\n",
    "### Approach 3\n",
    "\n",
    "$x^{\\prime} = r\\left(x + \\Delta t f(t, x; \\theta) \\right)$\n",
    "where $r^{\\alpha}(x) := \\left( x^{\\alpha} - min(x) \\right) / \\left( max(x) - min(x) \\right)$, and $x$ is initialized s.t. $min(x) = 0$ and $max(x) = 1$. The max and min functions are in soft version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References:\n",
    "\n",
    "1. [Chaos appears in RGE (as a high dimensional non-linear ODE)](https://physics.stackexchange.com/a/55057) (the [paper](https://arxiv.org/abs/hep-th/0304178) related)."
   ]
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
